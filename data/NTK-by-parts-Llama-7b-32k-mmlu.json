{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-anatomy": {
      "acc": 0.37037037037037035,
      "acc_stderr": 0.041716541613545426,
      "acc_norm": 0.37037037037037035,
      "acc_norm_stderr": 0.041716541613545426
    },
    "hendrycksTest-astronomy": {
      "acc": 0.24342105263157895,
      "acc_stderr": 0.034923496688842384,
      "acc_norm": 0.24342105263157895,
      "acc_norm_stderr": 0.034923496688842384
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.2943396226415094,
      "acc_stderr": 0.02804918631569525,
      "acc_norm": 0.2943396226415094,
      "acc_norm_stderr": 0.02804918631569525
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3055555555555556,
      "acc_stderr": 0.03852084696008534,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.03852084696008534
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.22,
      "acc_stderr": 0.0416333199893227,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.0416333199893227
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.2832369942196532,
      "acc_stderr": 0.034355680560478746,
      "acc_norm": 0.2832369942196532,
      "acc_norm_stderr": 0.034355680560478746
    },
    "hendrycksTest-college_physics": {
      "acc": 0.16666666666666666,
      "acc_stderr": 0.037082846624165444,
      "acc_norm": 0.16666666666666666,
      "acc_norm_stderr": 0.037082846624165444
    },
    "hendrycksTest-computer_security": {
      "acc": 0.41,
      "acc_stderr": 0.04943110704237102,
      "acc_norm": 0.41,
      "acc_norm_stderr": 0.04943110704237102
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3617021276595745,
      "acc_stderr": 0.03141082197596239,
      "acc_norm": 0.3617021276595745,
      "acc_norm_stderr": 0.03141082197596239
    },
    "hendrycksTest-econometrics": {
      "acc": 0.22807017543859648,
      "acc_stderr": 0.03947152782669415,
      "acc_norm": 0.22807017543859648,
      "acc_norm_stderr": 0.03947152782669415
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.2620689655172414,
      "acc_stderr": 0.036646663372252565,
      "acc_norm": 0.2620689655172414,
      "acc_norm_stderr": 0.036646663372252565
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2619047619047619,
      "acc_stderr": 0.022644212615525214,
      "acc_norm": 0.2619047619047619,
      "acc_norm_stderr": 0.022644212615525214
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.30952380952380953,
      "acc_stderr": 0.04134913018303316,
      "acc_norm": 0.30952380952380953,
      "acc_norm_stderr": 0.04134913018303316
    },
    "hendrycksTest-global_facts": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252605,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252605
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3032258064516129,
      "acc_stderr": 0.026148685930671746,
      "acc_norm": 0.3032258064516129,
      "acc_norm_stderr": 0.026148685930671746
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.24630541871921183,
      "acc_stderr": 0.03031509928561773,
      "acc_norm": 0.24630541871921183,
      "acc_norm_stderr": 0.03031509928561773
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542128,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542128
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.4121212121212121,
      "acc_stderr": 0.03843566993588717,
      "acc_norm": 0.4121212121212121,
      "acc_norm_stderr": 0.03843566993588717
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.25252525252525254,
      "acc_stderr": 0.030954055470365904,
      "acc_norm": 0.25252525252525254,
      "acc_norm_stderr": 0.030954055470365904
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.38341968911917096,
      "acc_stderr": 0.03508984236295342,
      "acc_norm": 0.38341968911917096,
      "acc_norm_stderr": 0.03508984236295342
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.30512820512820515,
      "acc_stderr": 0.023346335293325884,
      "acc_norm": 0.30512820512820515,
      "acc_norm_stderr": 0.023346335293325884
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.026067159222275805,
      "acc_norm": 0.24074074074074073,
      "acc_norm_stderr": 0.026067159222275805
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.28991596638655465,
      "acc_stderr": 0.029472485833136094,
      "acc_norm": 0.28991596638655465,
      "acc_norm_stderr": 0.029472485833136094
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2913907284768212,
      "acc_stderr": 0.03710185726119994,
      "acc_norm": 0.2913907284768212,
      "acc_norm_stderr": 0.03710185726119994
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.3174311926605505,
      "acc_stderr": 0.0199571521984605,
      "acc_norm": 0.3174311926605505,
      "acc_norm_stderr": 0.0199571521984605
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.03167468706828979,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.03167468706828979
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.4215686274509804,
      "acc_stderr": 0.03465868196380758,
      "acc_norm": 0.4215686274509804,
      "acc_norm_stderr": 0.03465868196380758
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.4008438818565401,
      "acc_stderr": 0.031900803894732356,
      "acc_norm": 0.4008438818565401,
      "acc_norm_stderr": 0.031900803894732356
    },
    "hendrycksTest-human_aging": {
      "acc": 0.3632286995515695,
      "acc_stderr": 0.032277904428505,
      "acc_norm": 0.3632286995515695,
      "acc_norm_stderr": 0.032277904428505
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.3282442748091603,
      "acc_stderr": 0.04118438565806298,
      "acc_norm": 0.3282442748091603,
      "acc_norm_stderr": 0.04118438565806298
    },
    "hendrycksTest-international_law": {
      "acc": 0.4297520661157025,
      "acc_stderr": 0.04519082021319773,
      "acc_norm": 0.4297520661157025,
      "acc_norm_stderr": 0.04519082021319773
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.4166666666666667,
      "acc_stderr": 0.04766075165356461,
      "acc_norm": 0.4166666666666667,
      "acc_norm_stderr": 0.04766075165356461
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.3558282208588957,
      "acc_stderr": 0.03761521380046734,
      "acc_norm": 0.3558282208588957,
      "acc_norm_stderr": 0.03761521380046734
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.25892857142857145,
      "acc_stderr": 0.041577515398656284,
      "acc_norm": 0.25892857142857145,
      "acc_norm_stderr": 0.041577515398656284
    },
    "hendrycksTest-management": {
      "acc": 0.30097087378640774,
      "acc_stderr": 0.045416094465039455,
      "acc_norm": 0.30097087378640774,
      "acc_norm_stderr": 0.045416094465039455
    },
    "hendrycksTest-marketing": {
      "acc": 0.405982905982906,
      "acc_stderr": 0.03217180182641087,
      "acc_norm": 0.405982905982906,
      "acc_norm_stderr": 0.03217180182641087
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252604,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252604
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.41379310344827586,
      "acc_stderr": 0.017612204084663772,
      "acc_norm": 0.41379310344827586,
      "acc_norm_stderr": 0.017612204084663772
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.36127167630057805,
      "acc_stderr": 0.025862201852277895,
      "acc_norm": 0.36127167630057805,
      "acc_norm_stderr": 0.025862201852277895
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2424581005586592,
      "acc_stderr": 0.014333522059217889,
      "acc_norm": 0.2424581005586592,
      "acc_norm_stderr": 0.014333522059217889
    },
    "hendrycksTest-nutrition": {
      "acc": 0.35294117647058826,
      "acc_stderr": 0.027363593284684937,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.027363593284684937
    },
    "hendrycksTest-philosophy": {
      "acc": 0.31189710610932475,
      "acc_stderr": 0.02631185807185416,
      "acc_norm": 0.31189710610932475,
      "acc_norm_stderr": 0.02631185807185416
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3395061728395062,
      "acc_stderr": 0.026348564412011624,
      "acc_norm": 0.3395061728395062,
      "acc_norm_stderr": 0.026348564412011624
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.29432624113475175,
      "acc_stderr": 0.027187127011503796,
      "acc_norm": 0.29432624113475175,
      "acc_norm_stderr": 0.027187127011503796
    },
    "hendrycksTest-professional_law": {
      "acc": 0.27835723598435463,
      "acc_stderr": 0.011446990197380984,
      "acc_norm": 0.27835723598435463,
      "acc_norm_stderr": 0.011446990197380984
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.40808823529411764,
      "acc_stderr": 0.029855261393483927,
      "acc_norm": 0.40808823529411764,
      "acc_norm_stderr": 0.029855261393483927
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3284313725490196,
      "acc_stderr": 0.018999707383162673,
      "acc_norm": 0.3284313725490196,
      "acc_norm_stderr": 0.018999707383162673
    },
    "hendrycksTest-public_relations": {
      "acc": 0.2818181818181818,
      "acc_stderr": 0.043091187099464585,
      "acc_norm": 0.2818181818181818,
      "acc_norm_stderr": 0.043091187099464585
    },
    "hendrycksTest-security_studies": {
      "acc": 0.3469387755102041,
      "acc_stderr": 0.030472526026726492,
      "acc_norm": 0.3469387755102041,
      "acc_norm_stderr": 0.030472526026726492
    },
    "hendrycksTest-sociology": {
      "acc": 0.44776119402985076,
      "acc_stderr": 0.03516184772952167,
      "acc_norm": 0.44776119402985076,
      "acc_norm_stderr": 0.03516184772952167
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-virology": {
      "acc": 0.3373493975903614,
      "acc_stderr": 0.03680783690727581,
      "acc_norm": 0.3373493975903614,
      "acc_norm_stderr": 0.03680783690727581
    },
    "hendrycksTest-world_religions": {
      "acc": 0.45614035087719296,
      "acc_stderr": 0.03820042586602967,
      "acc_norm": 0.45614035087719296,
      "acc_norm_stderr": 0.03820042586602967
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=emozilla/NTK-by-parts-Llama-7b-32k,use_accelerate=True,dtype=bfloat16,trust_remote_code=True",
    "num_fewshot": 5,
    "batch_size": "5",
    "batch_sizes": [],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}