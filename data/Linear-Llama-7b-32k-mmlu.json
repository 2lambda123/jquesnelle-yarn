{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.2,
      "acc_stderr": 0.04020151261036846,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.04020151261036846
    },
    "hendrycksTest-anatomy": {
      "acc": 0.28888888888888886,
      "acc_stderr": 0.03915450630414251,
      "acc_norm": 0.28888888888888886,
      "acc_norm_stderr": 0.03915450630414251
    },
    "hendrycksTest-astronomy": {
      "acc": 0.19078947368421054,
      "acc_stderr": 0.031975658210325,
      "acc_norm": 0.19078947368421054,
      "acc_norm_stderr": 0.031975658210325
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.24528301886792453,
      "acc_stderr": 0.026480357179895702,
      "acc_norm": 0.24528301886792453,
      "acc_norm_stderr": 0.026480357179895702
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2708333333333333,
      "acc_stderr": 0.03716177437566017,
      "acc_norm": 0.2708333333333333,
      "acc_norm_stderr": 0.03716177437566017
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.22,
      "acc_stderr": 0.04163331998932269,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.04163331998932269
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909281,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909281
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.2254335260115607,
      "acc_stderr": 0.031862098516411454,
      "acc_norm": 0.2254335260115607,
      "acc_norm_stderr": 0.031862098516411454
    },
    "hendrycksTest-college_physics": {
      "acc": 0.20588235294117646,
      "acc_stderr": 0.04023382273617747,
      "acc_norm": 0.20588235294117646,
      "acc_norm_stderr": 0.04023382273617747
    },
    "hendrycksTest-computer_security": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3021276595744681,
      "acc_stderr": 0.030017554471880557,
      "acc_norm": 0.3021276595744681,
      "acc_norm_stderr": 0.030017554471880557
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2894736842105263,
      "acc_stderr": 0.04266339443159394,
      "acc_norm": 0.2894736842105263,
      "acc_norm_stderr": 0.04266339443159394
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.23448275862068965,
      "acc_stderr": 0.035306258743465914,
      "acc_norm": 0.23448275862068965,
      "acc_norm_stderr": 0.035306258743465914
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2566137566137566,
      "acc_stderr": 0.022494510767503154,
      "acc_norm": 0.2566137566137566,
      "acc_norm_stderr": 0.022494510767503154
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.20634920634920634,
      "acc_stderr": 0.0361960452412425,
      "acc_norm": 0.20634920634920634,
      "acc_norm_stderr": 0.0361960452412425
    },
    "hendrycksTest-global_facts": {
      "acc": 0.16,
      "acc_stderr": 0.03684529491774708,
      "acc_norm": 0.16,
      "acc_norm_stderr": 0.03684529491774708
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3032258064516129,
      "acc_stderr": 0.026148685930671742,
      "acc_norm": 0.3032258064516129,
      "acc_norm_stderr": 0.026148685930671742
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.21182266009852216,
      "acc_stderr": 0.028748983689941075,
      "acc_norm": 0.21182266009852216,
      "acc_norm_stderr": 0.028748983689941075
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.3212121212121212,
      "acc_stderr": 0.03646204963253812,
      "acc_norm": 0.3212121212121212,
      "acc_norm_stderr": 0.03646204963253812
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.19696969696969696,
      "acc_stderr": 0.028335609732463355,
      "acc_norm": 0.19696969696969696,
      "acc_norm_stderr": 0.028335609732463355
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.21243523316062177,
      "acc_stderr": 0.02951928261681724,
      "acc_norm": 0.21243523316062177,
      "acc_norm_stderr": 0.02951928261681724
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.28205128205128205,
      "acc_stderr": 0.022815813098896628,
      "acc_norm": 0.28205128205128205,
      "acc_norm_stderr": 0.022815813098896628
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.027195934804085622,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.027195934804085622
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.25210084033613445,
      "acc_stderr": 0.028205545033277723,
      "acc_norm": 0.25210084033613445,
      "acc_norm_stderr": 0.028205545033277723
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2185430463576159,
      "acc_stderr": 0.03374235550425694,
      "acc_norm": 0.2185430463576159,
      "acc_norm_stderr": 0.03374235550425694
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.25137614678899084,
      "acc_stderr": 0.01859920636028741,
      "acc_norm": 0.25137614678899084,
      "acc_norm_stderr": 0.01859920636028741
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3055555555555556,
      "acc_stderr": 0.03141554629402543,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.03141554629402543
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.2696078431372549,
      "acc_stderr": 0.03114557065948678,
      "acc_norm": 0.2696078431372549,
      "acc_norm_stderr": 0.03114557065948678
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.3459915611814346,
      "acc_stderr": 0.030964810588786713,
      "acc_norm": 0.3459915611814346,
      "acc_norm_stderr": 0.030964810588786713
    },
    "hendrycksTest-human_aging": {
      "acc": 0.29596412556053814,
      "acc_stderr": 0.030636591348699796,
      "acc_norm": 0.29596412556053814,
      "acc_norm_stderr": 0.030636591348699796
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.22137404580152673,
      "acc_stderr": 0.036412970813137296,
      "acc_norm": 0.22137404580152673,
      "acc_norm_stderr": 0.036412970813137296
    },
    "hendrycksTest-international_law": {
      "acc": 0.371900826446281,
      "acc_stderr": 0.04412015806624505,
      "acc_norm": 0.371900826446281,
      "acc_norm_stderr": 0.04412015806624505
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.0413311944024384,
      "acc_norm": 0.24074074074074073,
      "acc_norm_stderr": 0.0413311944024384
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.3374233128834356,
      "acc_stderr": 0.03714908409935573,
      "acc_norm": 0.3374233128834356,
      "acc_norm_stderr": 0.03714908409935573
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.32142857142857145,
      "acc_stderr": 0.044328040552915206,
      "acc_norm": 0.32142857142857145,
      "acc_norm_stderr": 0.044328040552915206
    },
    "hendrycksTest-management": {
      "acc": 0.1553398058252427,
      "acc_stderr": 0.03586594738573972,
      "acc_norm": 0.1553398058252427,
      "acc_norm_stderr": 0.03586594738573972
    },
    "hendrycksTest-marketing": {
      "acc": 0.21794871794871795,
      "acc_stderr": 0.027046857630716657,
      "acc_norm": 0.21794871794871795,
      "acc_norm_stderr": 0.027046857630716657
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.2835249042145594,
      "acc_stderr": 0.016117318166832283,
      "acc_norm": 0.2835249042145594,
      "acc_norm_stderr": 0.016117318166832283
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.2774566473988439,
      "acc_stderr": 0.024105712607754307,
      "acc_norm": 0.2774566473988439,
      "acc_norm_stderr": 0.024105712607754307
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24692737430167597,
      "acc_stderr": 0.014422292204808835,
      "acc_norm": 0.24692737430167597,
      "acc_norm_stderr": 0.014422292204808835
    },
    "hendrycksTest-nutrition": {
      "acc": 0.25163398692810457,
      "acc_stderr": 0.024848018263875195,
      "acc_norm": 0.25163398692810457,
      "acc_norm_stderr": 0.024848018263875195
    },
    "hendrycksTest-philosophy": {
      "acc": 0.22508038585209003,
      "acc_stderr": 0.023720088516179034,
      "acc_norm": 0.22508038585209003,
      "acc_norm_stderr": 0.023720088516179034
    },
    "hendrycksTest-prehistory": {
      "acc": 0.23765432098765432,
      "acc_stderr": 0.023683591837008553,
      "acc_norm": 0.23765432098765432,
      "acc_norm_stderr": 0.023683591837008553
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2375886524822695,
      "acc_stderr": 0.025389512552729906,
      "acc_norm": 0.2375886524822695,
      "acc_norm_stderr": 0.025389512552729906
    },
    "hendrycksTest-professional_law": {
      "acc": 0.24967405475880053,
      "acc_stderr": 0.011054538377832329,
      "acc_norm": 0.24967405475880053,
      "acc_norm_stderr": 0.011054538377832329
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4485294117647059,
      "acc_stderr": 0.030211479609121596,
      "acc_norm": 0.4485294117647059,
      "acc_norm_stderr": 0.030211479609121596
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.25163398692810457,
      "acc_stderr": 0.017555818091322263,
      "acc_norm": 0.25163398692810457,
      "acc_norm_stderr": 0.017555818091322263
    },
    "hendrycksTest-public_relations": {
      "acc": 0.22727272727272727,
      "acc_stderr": 0.04013964554072775,
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.04013964554072775
    },
    "hendrycksTest-security_studies": {
      "acc": 0.2163265306122449,
      "acc_stderr": 0.026358916334904024,
      "acc_norm": 0.2163265306122449,
      "acc_norm_stderr": 0.026358916334904024
    },
    "hendrycksTest-sociology": {
      "acc": 0.25870646766169153,
      "acc_stderr": 0.03096590312357301,
      "acc_norm": 0.25870646766169153,
      "acc_norm_stderr": 0.03096590312357301
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768081,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768081
    },
    "hendrycksTest-virology": {
      "acc": 0.24096385542168675,
      "acc_stderr": 0.0332939411907353,
      "acc_norm": 0.24096385542168675,
      "acc_norm_stderr": 0.0332939411907353
    },
    "hendrycksTest-world_religions": {
      "acc": 0.22807017543859648,
      "acc_stderr": 0.03218093795602357,
      "acc_norm": 0.22807017543859648,
      "acc_norm_stderr": 0.03218093795602357
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=emozilla/Linear-Llama-7b-32k,use_accelerate=True,dtype=bfloat16,trust_remote_code=True",
    "num_fewshot": 5,
    "batch_size": "5",
    "batch_sizes": [],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}