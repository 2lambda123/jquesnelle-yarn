{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.24,
      "acc_stderr": 0.042923469599092816,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.042923469599092816
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04072314811876837,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04072314811876837
    },
    "hendrycksTest-astronomy": {
      "acc": 0.19078947368421054,
      "acc_stderr": 0.031975658210325,
      "acc_norm": 0.19078947368421054,
      "acc_norm_stderr": 0.031975658210325
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421255,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421255
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.2981132075471698,
      "acc_stderr": 0.028152837942493864,
      "acc_norm": 0.2981132075471698,
      "acc_norm_stderr": 0.028152837942493864
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2916666666666667,
      "acc_stderr": 0.03800968060554858,
      "acc_norm": 0.2916666666666667,
      "acc_norm_stderr": 0.03800968060554858
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.19,
      "acc_stderr": 0.039427724440366255,
      "acc_norm": 0.19,
      "acc_norm_stderr": 0.039427724440366255
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621504,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621504
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.27,
      "acc_stderr": 0.0446196043338474,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.0446196043338474
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.28901734104046245,
      "acc_stderr": 0.03456425745087001,
      "acc_norm": 0.28901734104046245,
      "acc_norm_stderr": 0.03456425745087001
    },
    "hendrycksTest-college_physics": {
      "acc": 0.19607843137254902,
      "acc_stderr": 0.03950581861179962,
      "acc_norm": 0.19607843137254902,
      "acc_norm_stderr": 0.03950581861179962
    },
    "hendrycksTest-computer_security": {
      "acc": 0.35,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.2978723404255319,
      "acc_stderr": 0.029896145682095462,
      "acc_norm": 0.2978723404255319,
      "acc_norm_stderr": 0.029896145682095462
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2807017543859649,
      "acc_stderr": 0.042270544512322004,
      "acc_norm": 0.2807017543859649,
      "acc_norm_stderr": 0.042270544512322004
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.22758620689655173,
      "acc_stderr": 0.03493950380131184,
      "acc_norm": 0.22758620689655173,
      "acc_norm_stderr": 0.03493950380131184
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2698412698412698,
      "acc_stderr": 0.022860838309232072,
      "acc_norm": 0.2698412698412698,
      "acc_norm_stderr": 0.022860838309232072
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.20634920634920634,
      "acc_stderr": 0.0361960452412425,
      "acc_norm": 0.20634920634920634,
      "acc_norm_stderr": 0.0361960452412425
    },
    "hendrycksTest-global_facts": {
      "acc": 0.22,
      "acc_stderr": 0.0416333199893227,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.0416333199893227
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.27419354838709675,
      "acc_stderr": 0.025378139970885203,
      "acc_norm": 0.27419354838709675,
      "acc_norm_stderr": 0.025378139970885203
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3251231527093596,
      "acc_stderr": 0.03295797566311271,
      "acc_norm": 0.3251231527093596,
      "acc_norm_stderr": 0.03295797566311271
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.41818181818181815,
      "acc_stderr": 0.03851716319398393,
      "acc_norm": 0.41818181818181815,
      "acc_norm_stderr": 0.03851716319398393
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.3484848484848485,
      "acc_stderr": 0.033948539651564025,
      "acc_norm": 0.3484848484848485,
      "acc_norm_stderr": 0.033948539651564025
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.39378238341968913,
      "acc_stderr": 0.035260770955482364,
      "acc_norm": 0.39378238341968913,
      "acc_norm_stderr": 0.035260770955482364
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.25384615384615383,
      "acc_stderr": 0.022066054378726257,
      "acc_norm": 0.25384615384615383,
      "acc_norm_stderr": 0.022066054378726257
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2851851851851852,
      "acc_stderr": 0.027528599210340492,
      "acc_norm": 0.2851851851851852,
      "acc_norm_stderr": 0.027528599210340492
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.2815126050420168,
      "acc_stderr": 0.029213549414372153,
      "acc_norm": 0.2815126050420168,
      "acc_norm_stderr": 0.029213549414372153
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.304635761589404,
      "acc_stderr": 0.03757949922943343,
      "acc_norm": 0.304635761589404,
      "acc_norm_stderr": 0.03757949922943343
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.3192660550458716,
      "acc_stderr": 0.019987829069750006,
      "acc_norm": 0.3192660550458716,
      "acc_norm_stderr": 0.019987829069750006
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.36574074074074076,
      "acc_stderr": 0.03284738857647207,
      "acc_norm": 0.36574074074074076,
      "acc_norm_stderr": 0.03284738857647207
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.28431372549019607,
      "acc_stderr": 0.03166009679399812,
      "acc_norm": 0.28431372549019607,
      "acc_norm_stderr": 0.03166009679399812
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.3080168776371308,
      "acc_stderr": 0.0300523893356057,
      "acc_norm": 0.3080168776371308,
      "acc_norm_stderr": 0.0300523893356057
    },
    "hendrycksTest-human_aging": {
      "acc": 0.3094170403587444,
      "acc_stderr": 0.03102441174057221,
      "acc_norm": 0.3094170403587444,
      "acc_norm_stderr": 0.03102441174057221
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.24427480916030533,
      "acc_stderr": 0.037683359597287434,
      "acc_norm": 0.24427480916030533,
      "acc_norm_stderr": 0.037683359597287434
    },
    "hendrycksTest-international_law": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.04391326286724071,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.04391326286724071
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.25,
      "acc_stderr": 0.04186091791394607,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04186091791394607
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.294478527607362,
      "acc_stderr": 0.03581165790474082,
      "acc_norm": 0.294478527607362,
      "acc_norm_stderr": 0.03581165790474082
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.16964285714285715,
      "acc_stderr": 0.0356236785009539,
      "acc_norm": 0.16964285714285715,
      "acc_norm_stderr": 0.0356236785009539
    },
    "hendrycksTest-management": {
      "acc": 0.2524271844660194,
      "acc_stderr": 0.04301250399690877,
      "acc_norm": 0.2524271844660194,
      "acc_norm_stderr": 0.04301250399690877
    },
    "hendrycksTest-marketing": {
      "acc": 0.2863247863247863,
      "acc_stderr": 0.029614323690456648,
      "acc_norm": 0.2863247863247863,
      "acc_norm_stderr": 0.029614323690456648
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.3384418901660281,
      "acc_stderr": 0.01692086958621067,
      "acc_norm": 0.3384418901660281,
      "acc_norm_stderr": 0.01692086958621067
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3063583815028902,
      "acc_stderr": 0.024818350129436593,
      "acc_norm": 0.3063583815028902,
      "acc_norm_stderr": 0.024818350129436593
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24692737430167597,
      "acc_stderr": 0.014422292204808868,
      "acc_norm": 0.24692737430167597,
      "acc_norm_stderr": 0.014422292204808868
    },
    "hendrycksTest-nutrition": {
      "acc": 0.2908496732026144,
      "acc_stderr": 0.026004800363952113,
      "acc_norm": 0.2908496732026144,
      "acc_norm_stderr": 0.026004800363952113
    },
    "hendrycksTest-philosophy": {
      "acc": 0.26688102893890675,
      "acc_stderr": 0.025122637608816643,
      "acc_norm": 0.26688102893890675,
      "acc_norm_stderr": 0.025122637608816643
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3055555555555556,
      "acc_stderr": 0.025630824975621344,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.025630824975621344
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.26595744680851063,
      "acc_stderr": 0.026358065698880592,
      "acc_norm": 0.26595744680851063,
      "acc_norm_stderr": 0.026358065698880592
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2803129074315515,
      "acc_stderr": 0.011471555944958616,
      "acc_norm": 0.2803129074315515,
      "acc_norm_stderr": 0.011471555944958616
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.44485294117647056,
      "acc_stderr": 0.030187532060329376,
      "acc_norm": 0.44485294117647056,
      "acc_norm_stderr": 0.030187532060329376
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.27450980392156865,
      "acc_stderr": 0.018054027458815198,
      "acc_norm": 0.27450980392156865,
      "acc_norm_stderr": 0.018054027458815198
    },
    "hendrycksTest-public_relations": {
      "acc": 0.2818181818181818,
      "acc_stderr": 0.04309118709946459,
      "acc_norm": 0.2818181818181818,
      "acc_norm_stderr": 0.04309118709946459
    },
    "hendrycksTest-security_studies": {
      "acc": 0.2653061224489796,
      "acc_stderr": 0.028263889943784603,
      "acc_norm": 0.2653061224489796,
      "acc_norm_stderr": 0.028263889943784603
    },
    "hendrycksTest-sociology": {
      "acc": 0.2537313432835821,
      "acc_stderr": 0.03076944496729601,
      "acc_norm": 0.2537313432835821,
      "acc_norm_stderr": 0.03076944496729601
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.29,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-virology": {
      "acc": 0.3253012048192771,
      "acc_stderr": 0.03647168523683226,
      "acc_norm": 0.3253012048192771,
      "acc_norm_stderr": 0.03647168523683226
    },
    "hendrycksTest-world_religions": {
      "acc": 0.2807017543859649,
      "acc_stderr": 0.034462962170884265,
      "acc_norm": 0.2807017543859649,
      "acc_norm_stderr": 0.034462962170884265
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=emozilla/NTK-Llama-7b-32k,use_accelerate=True,dtype=bfloat16,trust_remote_code=True",
    "num_fewshot": 5,
    "batch_size": "5",
    "batch_sizes": [],
    "device": null,
    "no_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}